
# 第5章:深入了解kafka

1. 集群成员关系
   1. 集群通过zookeeper维护成员关系，每个broker都有一个唯一id，通过在zookeeper上注册临时节点实现服务发现
   2. 关闭broker时，他注册的节点也会消失，但是唯一id会保存起来，如果启动另一个相同id的broker，数据也会保留下来
2. 控制器
   1. 第一个启动的broker会在zookeeper中注册一个/controller临时节点，并负责分区leader的选举
   2. 其它的broker也会在zookeeper中注册控制节点，但是会收到节点存在异常后退出
   3. 当控制器断开时其它broker会监听到zookeeper事件并尝试注册控制节点，注册成功的broker会生成一个新的值更大的epoch消息，以使其它broker忽略之前epoch的控制器消息
   4. 失去leader的分区会重新分配leader，一般为分区列表中的下一个副本
   5. 控制器通过epoch避免脑裂
3. 复制
   1. 首领副本
      1. 为保证一致性，所有生产者消费者的请求都会通过首领副本
   2. 跟随者副本
      1. 跟随者副本不会处理客户端请求，只会从首领副本处复制消息
      2. 如果首领副本崩溃，将会从跟随者副本选举新的首领副本
   3. 跟随者同步数据是顺序的，没有完全同步数据的跟随者副本无法成为新的首领
   4. 首选首领
      1. 每个topic会有一个首选首领，首选首领是通过负载均衡后得到的，如果为同步状态会优先变成首领
4. 处理请求
   1. broker提供了二进制协议来指定请求消息的格式和如果响应消息
   2. 消息头
      1. request type(API key)
      2. request version(broker 可以处理不同版本的客户端请求)
      3. correcorrelation id (具有唯一性的数字,用于标示请求消息,同时也会出现在响应消息与错误日志中,方便定位问题)
      4. client id (用于标示发送请求的客户端)
   3. 处理请求的流程
      1. broker在每个监听的端口上运行一个acceptor线程,这个线程会创建链接,并把它交给processes线程(也叫网络线程)处理
      2. 网络线程会将请求放入请求队列,并从响应队列获取响应返回给客户端
      3. IO线程会处理请求队列中的请求
         1. 生产请求,生产者发送的消息,它包含客户端要写入broker的消息
         2. 获取请求,消费者和跟随者副本需要从broker中读取消息时发送的请求
         3. 元数据请求,客户端获取对应topic的分区、副本等信息
         4. 生产和获取都需要发送给topic的leader上，否则会返回"非分区首领错误"的响应，收到该错误后客户端将重新请求元数据
   4. 生产请求
      1. 根据ack的配置决定是否写入成功，1:leader收到 all: 所有副本 0:不等待
      2. broker收到请求后会验证如下问题
         1. 客户端是否有topic写入权限
         2. ack属性的取值是否有效
         3. 如果ack=all时，是否有足够多的副本保证数据安全写入，根据配置，副本数量不足时，kafka可以拒绝处理新消息
      3. 消息会被写到文件系统的缓存中，不会等待刷新到磁盘上
   5. 获取请求
      1. 客户端向broker请求指定分区上特定偏移量的消息
      2. 客户端可以指定broker最多从一个分区里返回多少条数据
      3. 如果请求的数据存在，broker会直接将文件里发送到网络通道，不经过任何缓冲区，从而避免了字节服务和缓存管理以提高性能
      4. 客户端也可以设置返回数据量下限，当数据足够大(或缓存时间足够长)时从broker返回给客户端，减少了网络开销
      5. 大部分客户端只能读取到已经成功写入同步副本的消息
      6. 如果消息复制过程变慢，消费者接收数据的速度也会变慢
5. 物理存储
   1. kafka配置中log.dirs指定数据目录
   2. log4j.properties中可以指定服务日志
   3. 分区分配
      1. 尽量将同一分区的不同副本分配到不同的broker上，如有机架配置则优先按机架分散分配
   4. 文件管理
      1. kafka按时间或文件大小来保留数据
      2. 每一个分区会分成若干片段，根据配置中时间或大小中较先满足的条件拆分片段
      3. 正在写入的片段成为活跃片段，活跃片段不会被删除
      4. broker会对每个分区的每个片段建立一个文件句柄，需要根据实际情况做调优
   5. 文件格式
      1. 保存在磁盘上的数据格式与生产者发送的数据一致，kafka服务解析，采用零复制技术向消费者发送数据，同时避免数据压缩与解压
      2. 数据中包含时间戳，时间可以是生产者发送数据的时间，也可以是数据到达broker的时间
      3. 压缩过的一条消息中可能包含多条数据，每条数据都有独立的偏移量和时间戳
   6. 索引
      1. 消费者可以从kafka的任意偏移量位置读取数据
      2. 每个分区会维护一个索引，索引把偏移量对应到具体数据文件的对应位置上
      3. 删除或损坏索引是安全的，kafka会重新读取文件重建索引
   7. 数据清理
      1. 一般情况下kafka会根据时间清理数据

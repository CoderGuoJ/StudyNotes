
# 中文分词

* ## 概述

   词是最小的能够独立活动的有意义的语言成分，一般分词是自然语言处理的第一项核心技术。英文中每个句子都将词用空格或标点符号分隔开来，而在中文中很难对词的边界进行界定，难以将词划分出来。在汉语中，虽然是以字为最小单位，但是一篇文章的语义表达却仍然是以词来划分的。因此处理中文文本时，需要进行分词处理，将句子转为词的表示，这就是中文分词。

* ## 中文分词的难题

   1. 分词规则

       构建完美的分词规则便可以将所有的句子正确的划分，但是这根本无法实现，语言是长期发展自然而然形成的，而且语言规则庞大复杂，很难做出完美的分词规则。

   2. 消除歧义

        在中文句子中，很多词是由歧义性的，在一句话也可能有多种分词方法。比如：”结婚/的/和尚/未结婚/的“，“结婚/的/和/尚未/结婚/的”，人分辨这样的句子都是问题，更何况是机器。

   3. 未登陆词

        没有被收录在分词词表中但必须切分出来的词，包括各类专有名词（人名、地名、企业名等）、缩写词、新增词汇等等

* ## 主流的分词方法

    1. 基于规则的分词
       1. 概述

           主要是人工建立词库也叫做词典，通过词典匹配的方式对句子进行划分。其实现简单高效，但是对未登陆词很难进行处理。主要有正向最大匹配法，逆向最大匹配法以及双向最大匹配法。
       2. 正向最大匹配法(FMM)

            从句子的左边向右查找，并逐一与词典匹配，匹配成功后尝试将下一个字合并入当前词，如果合并后依然可以匹配成功就以此类推，直到匹配失败时将最长的词记录，并从词尾处再进行下一次匹配
       3. 逆向最大匹配法(RMM)

            原理与FMM类似，只是方向相反。逆向匹配的切分精度略高于正向匹配，遇到的歧义现象也较少
       4. 双向最大匹配法(Bi-MM)

            将FMM与RMM结果进行比较，然后按照最大匹配原则，选取词数切分最少的作为结果。中文中90.0%左右的句子，正向最大匹配法和逆向最大匹配法完全重合且正确，只有大概9.0%的句子两种切分方法得到的结果不一样，但其中必有一个是正确的（歧义检测成功），只有不到1.0%的句子，使用正向最大匹配法和逆向最大匹配法的切分虽然重合但是错的，或者两种方法切分不同但结果都不对（歧义检测失败）。
       5. 双向最大匹配法的规则

            1. FMM与RMM分词结果词数不同时，选择分词数量较少的那个
            2. 分词结果相同，没有歧义，返回任意一个
            3. 分词结果不同，返回单字的词出现数量最少的那个
       6. 算法示意图
            ![avatar](/nlp/分词/基于规则的分词方法.png)
       7. 基于规则的分词算法优缺点
          1. 实现简单，效果尚可
          2. 速度快，都是O(n)时间复杂度
          3. 对于未登陆词效果很差，非常依赖于词典库
          4. 随着字典库的增大，词与词直接的交叉会越来越多，产生难以处理的歧义问题

    2. 基于统计的分词方法

        统计分词的主要思想是把每个词看做是由字组成的，如果相连的字权重越大，就证明这段相连的字很有可能就是一个词，权重计算会包括出现频率，上下文关联性等。jieba分词工具就是采用了基于统计的分词
       1. N最短路径

            利用动态规划思想，寻找处所有可能出现的分词结果，然后在这些结果中寻找一条权值和最大的路径, 路径上的边标志了最可能的分词结果.通常我们只寻找权值和最大的那一条路径。
       2. N最短路径示意图

           ![avatar](/nlp/分词/n最短路径分词.png)
       3. N元文法模型(N-Gram)

            N-Gram是通过词与词的关联关系来计算分词的，
            令C是待切分的汉字串，W={W1W2...Wn}．W 是切词的结果。
            设P(W|C)是汉字串C切分为W的某种估计概率。
            Wa，Wb，⋯．Wk是C的所有可能的切分方案。那么，基于统计的切分模型就是这样的一种分词模型，它能够找到目的词串W ，使得W满足：

            ```text
            P(W|C)=MAX(P(Wa|C),P(Wb|C)...P(Wk|C))，
            ```

            根据贝叶斯公式得:

            ```text
            P(W|C)=[P(W)P(C|W)]/P(C)
            ```

            且P(C)是一个常数,P(C|W)=1,所以只需要求出P(W)即可，根据乘法公式得

            ```text
            P(W)=P(w)P(w2|w1)P(w3|w1w2)⋯P(wk|w1,w2...wk-1)
            ```

            由此公式可计算得出某种拆词方法得估计概率，计算所有切分可能选择最大可能性得结果即可。但是直接求解的话计算复杂度很高，于是引入了"马尔可夫有限历史性假设"：

            ```text
            任意一个词Wi出现的概率最多只同它前n-1个有限的词有关,也就是n元
            ```

            于是公式可以简化为:

            ```text
            二元语法模型(一阶马尔可夫链 bi-gram):
            P(W)=P(w1)P(w2|w1)P(w3|w2)⋯P(wk|wk-1)
            三元语法模型(二阶马尔可夫链 tri-gram):
            P(W)=P(w1)P(w2|w1)P(w3|w2w1)⋯P(Wk|wk-1wk-2)
            ```

            使用较多的就是二元与三元语法模型，随着n的增大，计算复杂度会成指数级增长，像Google这种巨头，也只是用到了大约四元的程度，它对计算能力和空间的需求都太大了
       4. 隐式马尔可夫模型(HMM)

            前面介绍的几组切词方案都是基于词来切分的，而HMM是根据字切分的，是通过可以观测到的一些显性数据计算出与之相关的隐性数据的机器学习算法。这个模型由五元组构成：

            1. StatusSet: 状态值集合

               状态值集合为(B, M, E, S): {B:begin, M:middle, E:end, S:single}。分别代表每个状态代表的是该字在词语中的位置，B代表该字是词语中的起始字，M代表是词语中的中间字，E代表是词语中的结束字，S则代表是单字成词。
            2. ObservedSet: 观察值集合

               观察值集合为就是所有汉字(东南西北你我他…)，甚至包括标点符号所组成的集合
            3. TransProbMatrix: 转移概率矩阵

               指从某个状态转换到另一个状态的概率矩阵，根据有限历史性假设(当前词的状态只与前一个词的状态有关)。由此可以得出4*4矩阵
               ![avatar](/nlp/分词/转移概率矩阵.png)

            4. EmitProbMatrix: 发射概率矩阵

               根据观察值独立性假设(每个观察值的结果只与当前观察值的状态有关)，可以利用已知的样本经过统计求出每个状态值下各观察值出现的概率:

               ```text
               #B
               耀:-10.460283,涉:-8.766406,谈:-8.039065,伊:-7.682602,洞:-8.668696,...
               #E
               耀:-9.266706,涉:-9.096474,谈:-8.435707,伊:-10.223786,洞:-8.366213,...
               #M
               耀:-8.47651,涉:-10.560093,谈:-8.345223,伊:-8.021847,洞:-9.547990,....
               #S
               蘄:-10.005820,涉:-10.523076,唎:-15.269250,禑:-17.215160,洞:-8.369527...
               ```

            5. InitStatus: 初始状态分布
               第一个字的(B,M,E,S)分别出现的概率,如:

               ```text
               B：-0.26268660809250016
               E：-3.14e+100
               M：-3.14e+100
               S：-1.4652633398537678
               ```

               以上的数据是由概率取对数(可以让概率相乘的计算变成对数相加)的结果，其中-3.14e+100作为负无穷，也就是对应的概率值是0

            6. HMM模型可以用来解决三个问题:

               1. 参数(StatusSet, TransProbMatrix, EmitRobMatrix, InitStatus)已知的情况下，求解(ObservedSet)。(Forward-backward算法)
               2. 参数(ObservedSet, TransProbMatrix, EmitRobMatrix, InitStatus)已知的情况下，求解(StatusSet)。(viterbi算法)
               3. 参数(ObservedSet)已知的情况下，求解(TransProbMatrix, EmitRobMatrix, InitStatus)。(Baum-Welch算法)
            7. Viterbi算法

               根据HMM算法的需求，我们需要求出 max(P(S|O)):

               ```text
               P(S|O)
               = [P(O|S)P(S)]/P(O)
               由于P(O)是常数，所以可以忽略
               = P(O|S)P(S)

               其中 P(O|S)
                    = P(o1,..,on|s1,..,sn)
                    = P(o1|s1,..,sn)*..*P(on|s1,..,sn)
                    根据独立性假设得
                    = P(o1|s1)*..P(on|sn)
               其中 P(S)
                    = P(s1)*P(s2|s1)*P(s3|s2,s1)..*P(sn|s1,..,sn)
                    根据有限历史性假设
                    = P(s1)*P(s2|s1)*P(s3|s2)..*P(sn,sn-1)
               这时 P(O|S)P(S)
                    = P(s1)P(o1|s1)*P(s2|s1)*P(o2|s2)*P(S3|s2)*..*P(on|sn)*P(sn|sn-1)
                    ≈ P(o1|s1)*P(s2|s1)*P(o2|s2)*P(S3|s2)*..*P(on|sn)*P(sn|sn-1)
                    P(o1)(s1) 即 InitStatus
                    P(oi|si) 即 EmitProbMatrix
                    P(si|si-1) 即 TransProbMatrix

               我们将上式分段即可得到递推公式
                    当 i=1 时
                         P(si|oi)
                         = P(o1|s1)
                    当 i>1 时
                         P(si|oi)
                         = P(si-1|oi-1) * P(si|si-1) * P(oi|si)
               ```

               Viterbi算法一种动态规划算法，通过递推公式不断在多个同级别的局部最优解中选择一个最优解一直推广到全局最优解，最后根据路径矩阵回溯最优路径

               ```java
               //遍历句子，下标i从1开始是因为刚才初始化的时候已经对0初始化结束了
                    for(size_t i = 1; i < 15; i++)
                    {
                    // 遍历可能的状态
                    for(size_t j = 0; j < 4; j++)
                    {
                         weight[j][i] = MIN_DOUBLE;
                         path[j][i] = -1;
                         //遍历前一个字可能的状态
                         for(size_t k = 0; k < 4; k++)
                         {
                              //前一个字为status[k]的最大概率 * status[k] 到status[j] 的概率 * 当前字为status[j]的概率
                              double tmp = weight[k][i-1] + _transProb[k][j] + _emitProb[j][sentence[i]];
                              if(tmp > weight[j][i]) // 找出最大的weight[j][i]值
                              {
                                   weight[j][i] = tmp;
                                   path[j][i] = k;
                              }
                         }
                    }
                    }
               ```

               viterbi算法解析:
                    ![avatar](/nlp/分词/viterbi算法.png)
